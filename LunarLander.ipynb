{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876735fe",
   "metadata": {},
   "source": [
    "## Initializing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225908ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tejapotu/miniconda3/envs/rl/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\") \n",
    "env.reset(seed=42)\n",
    "\n",
    "# play one complete episode\n",
    "while True:\n",
    "    action = env.action_space.sample()  # Sample a random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)  # Take the action in the environment\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished\")\n",
    "        break\n",
    "\n",
    "env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3ebca",
   "metadata": {},
   "source": [
    "## Safe Agent (keep the lander from touching the ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29ef823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -260.82875108605117\n"
     ]
    }
   ],
   "source": [
    "class SafeAgent:\n",
    "    def act(self, observation):\n",
    "        # minimum height\n",
    "        MIN_HEIGHT = 1\n",
    "        \n",
    "        # if the lander is too low, apply upward force\n",
    "        if observation[1] < MIN_HEIGHT:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def play_episode(agent, env):\n",
    "    observation, info = env.reset(seed=42)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "agent = SafeAgent()\n",
    "total_reward = play_episode(agent, env)\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0053e45",
   "metadata": {},
   "source": [
    "## Stable Agent (Keeps the lander stable in air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9edb49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -549.4518126418703\n"
     ]
    }
   ],
   "source": [
    "class StableAgent:\n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "            It will operate via the following rules:\n",
    "\n",
    "            1. If below height of 1: action = 2 (main engine)\n",
    "            2. If angle is above π/50: action = 1 (fire right engine)\n",
    "            3. If angle is above π/50: action = 1 (fire left engine)\n",
    "            4. If x distance is above 0.4: action = 3 (fire left engine)\n",
    "            5. If x distance is below -0.4: action = 1 (fire left engine)\n",
    "            6. If below height of 1.5: action = 2 (main engine)\n",
    "            7. Else: action = 0 (do nothing)\n",
    "        \"\"\"\n",
    "        MIN_HEIGHT = 1\n",
    "        MAX_ANGLE = 3.14 / 50\n",
    "        MAX_X_DISTANCE = 0.4\n",
    "\n",
    "        if observation[1] < MIN_HEIGHT:\n",
    "            return 2\n",
    "        elif observation[6] > MAX_ANGLE:\n",
    "            return 1\n",
    "        elif observation[6] < -MAX_ANGLE:\n",
    "            return 3\n",
    "        elif observation[0] > MAX_X_DISTANCE:\n",
    "            return 3\n",
    "        elif observation[0] < -MAX_X_DISTANCE:\n",
    "            return 1\n",
    "        elif observation[1] < 1.5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "agent = StableAgent()\n",
    "total_reward = play_episode(agent, env)\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "env.close()  # Close the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb778a1",
   "metadata": {},
   "source": [
    "## Deep lerning method to land the lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1405aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN algorithm\n",
    "import torch\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        '''\n",
    "            Architecture of the DQN model:\n",
    "            1. Input layer\n",
    "            2. Hidden layer\n",
    "            3. Output layer\n",
    "        '''\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.layer1(state))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a1f48",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72306a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Push a transition into the replay buffer.\n",
    "        \n",
    "        :param state: The current state\n",
    "        :param action: The action taken\n",
    "        :param reward: The reward received\n",
    "        :param next_state: The next state after taking the action\n",
    "        :param done: Whether the episode has ended\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the replay buffer.\n",
    "        \n",
    "        :param batch_size: The number of transitions to sample\n",
    "        :return: tuple of numpy.ndarray - A batch of transitions\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the current size of the replay buffer.\n",
    "        \n",
    "        :return: The number of transitions in the buffer\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaed0f1",
   "metadata": {},
   "source": [
    "## Define the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe348e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64, learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent with the necessary parameters.\n",
    "        Args:\n",
    "            state_size (int): The size of the state space.\n",
    "            action_size (int): The size of the action space.\n",
    "            hidden_size (int): The size of the hidden layer in the DQN model.\n",
    "            learning_rate (float): The learning rate for the optimizer.\n",
    "            gamma (float): The discount factor for future rewards.\n",
    "            buffer_size (int): The size of the replay buffer.\n",
    "            batch_size (int): The size of the batch for training.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # define the target and online DQN networks\n",
    "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        \n",
    "        # Set weights of target network to be the same as those of the q network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store the transition in the replay buffer.\n",
    "        \n",
    "        :param state: The current state\n",
    "        :param action: The action taken\n",
    "        :param reward: The reward received\n",
    "        :param next_state: The next state after taking the action\n",
    "        :param done: Whether the episode has ended\n",
    "        \"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # if the buffer has enough samples, update the model\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.update_model()\n",
    "            \n",
    "    def act(self, state, epsilon):\n",
    "        '''\n",
    "            Choose an action based on the current state and the epsilon-greedy policy.\n",
    "            :param state: The current state\n",
    "            :param epsilon: The probability of choosing a random action\n",
    "        '''\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)      \n",
    "        \n",
    "        else:\n",
    "            # convert state to tensor and pass it through the q network\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            self.q_network.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state_tensor)\n",
    "            \n",
    "            self.q_network.train()\n",
    "            # get the action with the highest value    \n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "    def update_model(self):\n",
    "        \"\"\"\n",
    "        Update the Q-network using a batch of transitions from the replay buffer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # sample the batch from the replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # convert numpy arrays to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # compute the Q-values for the current states - forward pass through the q network\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # compute the Q-values for the next states - forward pass through the target network\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        \n",
    "        # compute the target Q-values\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = torch.nn.functional.mse_loss(q_values, target_q_values.detach())\n",
    "        \n",
    "        self.optimizer.zero_grad()  # zero the gradients\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update the target network by copying the weights from the Q-network.\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7247112",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb8cef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_agent(agent, env, n_episodes=2000, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update_freq=10):\n",
    "    \"\"\"\n",
    "    Train the DQN agent in the LunarLander environment.\n",
    "    \n",
    "    :param agent: The DQN agent\n",
    "    :param env: The LunarLander environment\n",
    "    :param n_episodes: The number of episodes to train for\n",
    "    :param epsilon_start: The initial value of epsilon for the epsilon-greedy policy\n",
    "    :param epsilon_end: The final value of epsilon for the epsilon-greedy policy\n",
    "    :param epsilon_decay: The decay rate for epsilon\n",
    "    :param target_update_freq: The frequency of updating the target network\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state, epsilon)\n",
    "            # take the action in the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        scores.append(total_reward)\n",
    "        scores_window.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        # print the progress\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Average Score: {np.mean(scores_window):.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        # update the target network\n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "        # stop training if the average score is above a threshold\n",
    "        if np.mean(scores_window) >= 200:\n",
    "            print(f\"Environment solved in {episode} episodes!\")\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "agent = DQNAgent(state_size=8, action_size=4)\n",
    "# scores = train_agent(agent, env)\n",
    "\n",
    "# load the trained model\n",
    "agent.q_network.load_state_dict(torch.load(\"q_network.pth\"))\n",
    "agent.target_network.load_state_dict(torch.load(\"target_network.pth\"))\n",
    "agent.q_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4326e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the models\n",
    "torch.save(agent.q_network.state_dict(), \"q_network.pth\")\n",
    "torch.save(agent.target_network.state_dict(), \"target_network.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477f1bc",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98bb373",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m         env.render()  \u001b[38;5;66;03m# render the environment\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_reward\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m total_reward = \u001b[43mplay_DQN_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m env.close()  \u001b[38;5;66;03m# Close the environment\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mplay_DQN_episode\u001b[39m\u001b[34m(env, agent)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     15\u001b[39m     action = agent.act(state, \u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# use epsilon=0 for testing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     next_state, reward, terminated, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m     19\u001b[39m     state = next_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.13/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.13/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.13/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.13/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.13/site-packages/gymnasium/envs/box2d/lunar_lander.py:665\u001b[39m, in \u001b[36mLunarLander.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    662\u001b[39m     reward = +\u001b[32m100\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(state, dtype=np.float32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.13/site-packages/gymnasium/envs/box2d/lunar_lander.py:778\u001b[39m, in \u001b[36mLunarLander.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    776\u001b[39m     \u001b[38;5;28mself\u001b[39m.screen.blit(\u001b[38;5;28mself\u001b[39m.surf, (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m    777\u001b[39m     pygame.event.pump()\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrender_fps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m     pygame.display.flip()\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "def play_DQN_episode(env, agent):\n",
    "    \"\"\"\n",
    "    Play a single episode using the trained DQN agent.\n",
    "    \n",
    "    :param env: The LunarLander environment\n",
    "    :param agent: The trained DQN agent\n",
    "    \"\"\"\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state, 0)  # use epsilon=0 for testing\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        env.render()  # render the environment\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "total_reward = play_DQN_episode(env, agent)\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383fec85",
   "metadata": {},
   "source": [
    "## Double DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901504ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64, learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent with the necessary parameters.\n",
    "        Args:\n",
    "            state_size (int): The size of the state space.\n",
    "            action_size (int): The size of the action space.\n",
    "            hidden_size (int): The size of the hidden layer in the DQN model.\n",
    "            learning_rate (float): The learning rate for the optimizer.\n",
    "            gamma (float): The discount factor for future rewards.\n",
    "            buffer_size (int): The size of the replay buffer.\n",
    "            batch_size (int): The size of the batch for training.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # define the target and online DQN networks\n",
    "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        \n",
    "        # Set weights of target network to be the same as those of the q network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store the transition in the replay buffer.\n",
    "        \n",
    "        :param state: The current state\n",
    "        :param action: The action taken\n",
    "        :param reward: The reward received\n",
    "        :param next_state: The next state after taking the action\n",
    "        :param done: Whether the episode has ended\n",
    "        \"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # if the buffer has enough samples, update the model\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.update_model()\n",
    "            \n",
    "    def act(self, state, epsilon):\n",
    "        '''\n",
    "            Choose an action based on the current state and the epsilon-greedy policy.\n",
    "            :param state: The current state\n",
    "            :param epsilon: The probability of choosing a random action\n",
    "        '''\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)      \n",
    "        \n",
    "        else:\n",
    "            # convert state to tensor and pass it through the q network\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            self.q_network.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state_tensor)\n",
    "            \n",
    "            self.q_network.train()\n",
    "            # get the action with the highest value    \n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "    def update_model(self):\n",
    "        \"\"\"\n",
    "        Update the Q-network using a batch of transitions from the replay buffer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # sample the batch from the replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # convert numpy arrays to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # compute the Q-values for the current states - forward pass through the q network\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # the only difference from DQN is here\n",
    "        # get the actions for the next states using the q network\n",
    "        next_actions = self.q_network(next_states).argmax(1)  # get the actions for the next states\n",
    "        # compute the Q-values for the next states - forward pass through the target network\n",
    "        next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # compute the target Q-values\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = torch.nn.functional.mse_loss(q_values, target_q_values.detach())\n",
    "        \n",
    "        self.optimizer.zero_grad()  # zero the gradients\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update the target network by copying the weights from the Q-network.\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "agent = DDQNAgent(state_size=8, action_size=4)\n",
    "scores = train_agent(agent, env)\n",
    "\n",
    "score = play_DQN_episode(env, agent)\n",
    "print(f\"Total reward: {score}\")\n",
    "env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd068a",
   "metadata": {},
   "source": [
    "## Dueling Deep Q-Networks (Dueling-DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb91049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
